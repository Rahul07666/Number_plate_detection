# -*- coding: utf-8 -*-
"""Rahul.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nNBGZAqVGkCKsvTWB6M696IPNT2cOgV_
"""

!pip install ultralytics

from google.colab import drive
drive.mount('/content/drive')

yaml_content = """
path: /content/drive/MyDrive/dataset
train: images/train
val: images/val
test: images/test

names:
  0: numberplate
"""

with open('/content/drive/MyDrive/dataset/data.yaml', 'w') as f:
    f.write(yaml_content)

from ultralytics import YOLO

model = YOLO('yolov8s.pt')  # or yolov8n.pt, yolov8m.pt, etc.

model.train(
    data='/content/drive/MyDrive/dataset/data.yaml',
    epochs=15,
    imgsz=640,
    batch=16,
    name='yolov8_number_plate',
    project='/content/drive/MyDrive/number_plate_project/yolov8_runs'
)

#!pip install ultralytics
from ultralytics import YOLO

# Load trained YOLOv8 model (coco has car, bike classes)
vehicle_model = YOLO('yolov8n.pt')  # Use 'yolov8s.pt' or 'yolov8m.pt' for better accuracy

import os
import random
import cv2
from matplotlib import pyplot as plt

import os
import random

# Define paths to the two folders
bike_folder = '/content/drive/MyDrive/bike_dataset1'
car_folder = '/content/drive/MyDrive/car dataset'

# Function to select a random video from a folder
def select_random_video(folder):
    if not os.path.exists(folder):
        raise FileNotFoundError(f"Folder not found: {folder}")
    videos = [f for f in os.listdir(folder) if f.lower().endswith(('.mp4', '.avi', '.mov'))]
    if not videos:
        raise ValueError(f"No video files found in: {folder}")
    return os.path.join(folder, random.choice(videos))

# Select 1 video from each folder
bike_video_path = select_random_video(bike_folder)
car_video_path = select_random_video(car_folder)

print(f"Selected Bike Video: {bike_video_path}")
print(f"Selected Car Video: {car_video_path}")

import cv2
import os

# Base directory to save frames
base_frames_dir = '/content/frames'
os.makedirs(base_frames_dir, exist_ok=True)

# Function to extract frames from a video
def extract_frames(video_path, save_dir_prefix):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise IOError(f"Failed to open video: {video_path}")

    # Create a folder for this video's frames
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    frames_dir = os.path.join(base_frames_dir, f'{save_dir_prefix}_{video_name}')
    os.makedirs(frames_dir, exist_ok=True)

    frame_count = 0
    frame_paths = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame_path = os.path.join(frames_dir, f'frame_{frame_count:04d}.jpg')
        cv2.imwrite(frame_path, frame)
        frame_paths.append(frame_path)
        frame_count += 1

    cap.release()
    print(f"Extracted {frame_count} frames from {video_path} to {frames_dir}")
    return frame_paths

# Extract frames from both videos
bike_frames = extract_frames(bike_video_path, 'bike')
car_frames = extract_frames(car_video_path, 'car')

import os
import cv2
from PIL import Image as PILImage
from ultralytics import YOLO

# === Config ===
bike_frames_dir = '/content/frames/bike_VID_20250714_201124'  # Replace <video_name> with actual
car_frames_dir = '/content/frames/car_VID_20250505_110654'    # Replace <video_name> with actual
FRAME_OFFSET_BIKE = -15
FRAME_OFFSET_CAR = -12

# Load YOLO model
vehicle_model = YOLO('yolov8n.pt')  # Or yolov8s/m for better accuracy

# === Utility: Get all frame paths from a directory ===
def load_frame_paths(frames_dir):
    frame_files = sorted(os.listdir(frames_dir))
    return [os.path.join(frames_dir, f) for f in frame_files if f.endswith('.jpg')]

# === Best vehicle frame selector with offset ===
def select_best_vehicle_frame(frame_paths, label, frame_offset):
    best_frame_idx = None
    largest_area = 0

    for idx, frame_path in enumerate(frame_paths):
        results = vehicle_model(frame_path)
        boxes = results[0].boxes

        for box in boxes:
            cls = int(box.cls[0])
            if cls in (2, 3):  # 2 = car, 3 = motorcycle
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                area = (x2 - x1) * (y2 - y1)
                if area > largest_area:
                    largest_area = area
                    best_frame_idx = idx

    if best_frame_idx is None:
        raise ValueError(f"No vehicle detected in any {label} frames.")

    # Apply frame offset within bounds
    target_idx = max(0, min(best_frame_idx + frame_offset, len(frame_paths) - 1))
    best_frame_path = frame_paths[target_idx]

    print(f" Selected best {label} frame (index {target_idx}): {best_frame_path}")
    display(PILImage.open(best_frame_path))
    return best_frame_path

# === Main Process ===
bike_frames = load_frame_paths(bike_frames_dir)
car_frames = load_frame_paths(car_frames_dir)

best_bike_frame_path = select_best_vehicle_frame(bike_frames, 'bike', FRAME_OFFSET_BIKE)
best_car_frame_path = select_best_vehicle_frame(car_frames, 'car', FRAME_OFFSET_CAR)

# Store for future use
best_frames = {
    "bike": best_bike_frame_path,
    "car": best_car_frame_path
}

from ultralytics import YOLO

# Load your trained number plate detection model
number_plate_model = YOLO('/content/drive/MyDrive/number_plate_project/yolov8_runs/yolov8_number_plate9/weights/best.pt')
# Run prediction on the best bike frame
bike_results = number_plate_model.predict(source=best_frames["bike"], conf=0.25, save=True)
print(f"Number plate detection completed for bike frame: {best_frames['bike']}")

#  Run prediction on the best car frame
car_results = number_plate_model.predict(source=best_frames["car"], conf=0.25, save=True)
print(f"Number plate detection completed for car frame: {best_frames['car']}")

from IPython.display import Image, display
import os
import glob

# === Get latest YOLO prediction directory ===
prediction_dirs = glob.glob('runs/detect/predict*')
latest_prediction_dir = max(prediction_dirs, key=os.path.getctime)

# === Display prediction result for bike ===
bike_filename = os.path.basename(best_frames['bike'])
bike_predicted_path = os.path.join(latest_prediction_dir, bike_filename)

print(f"Detected number plate in bike frame saved at: {bike_predicted_path}")
display(Image(filename=bike_predicted_path))

# === Display prediction result for car ===
car_filename = os.path.basename(best_frames['car'])
car_predicted_path = os.path.join(latest_prediction_dir, car_filename)

print(f"Detected number plate in car frame saved at: {car_predicted_path}")
display(Image(filename=car_predicted_path))

import os
import cv2

# Create cropped image output folder
cropped_dir = '/content/cropped_img'
os.makedirs(cropped_dir, exist_ok=True)

# Padding factor (10%)
padding_ratio = 0.1

# Function to crop from result
def crop_number_plates(results, frame_path, label):
    frame = cv2.imread(frame_path)
    frame_height, frame_width = frame.shape[:2]

    count = 0
    for result in results:
        boxes = result.boxes
        for box in boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())

            # Padding
            w, h = x2 - x1, y2 - y1
            pad_w, pad_h = int(w * padding_ratio), int(h * padding_ratio)

            # Padded coordinates (clipped)
            x1_p = max(0, x1 - pad_w)
            y1_p = max(0, y1 - pad_h)
            x2_p = min(frame_width, x2 + pad_w)
            y2_p = min(frame_height, y2 + pad_h)

            # Crop and save
            cropped_img = frame[y1_p:y2_p, x1_p:x2_p]
            crop_path = os.path.join(cropped_dir, f'{label}_plate_crop_{count}.jpg')
            cv2.imwrite(crop_path, cropped_img)
            print(f'Cropped number plate saved at: {crop_path}')
            count += 1

# Crop plates from both bike and car best frames
crop_number_plates(bike_results, best_frames["bike"], label="bike")
crop_number_plates(car_results, best_frames["car"], label="car")

import os
import pandas as pd

# === Config
root_folder = '/content/drive/MyDrive/char_images/train'  # May contain folders like A/, B/, 1/, etc.
output_csv = '/content/drive/MyDrive/char_images/labels.csv'

# === Collect images recursively
data = []
for root, dirs, files in os.walk(root_folder):
    for file in files:
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            label = os.path.basename(root)  # parent folder name = label
            file_path = os.path.join(root, file)
            rel_path = os.path.relpath(file_path, root_folder)  # relative path inside train/
            data.append({'filename': rel_path, 'label': label})

# === Save CSV
df = pd.DataFrame(data)
df.to_csv(output_csv, index=False)
print(f" Generated: {output_csv} with {len(df)} labeled entries")
print(df.head())

import os
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader

# === Config
# Corrected image_dir to point to the root of the training images
image_dir = '/content/drive/MyDrive/char_images/train'
csv_path = '/content/drive/MyDrive/char_images/labels.csv'
model_path = '/content/ocr_model_ecocr.pth'
classes_path = '/content/ocr_classes.txt'
batch_size = 32
epochs = 50
img_size = (32, 128)  # Wider images for better OCR
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === Load CSV
df = pd.read_csv(csv_path)
char_classes = sorted(df['label'].unique())

# Save class list
with open(classes_path, 'w') as f:
    for c in char_classes:
        f.write(f"{c}\n")

class_to_idx = {c: i for i, c in enumerate(char_classes)}
idx_to_class = {i: c for c, i in class_to_idx.items()}

# === Transform
transform = transforms.Compose([
    transforms.Resize(img_size),
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# === Custom Dataset
class OCRDataset(Dataset):
    def __init__(self, df, root_dir, transform, class_to_idx):
        self.df = df
        self.root_dir = root_dir
        self.transform = transform
        self.class_to_idx = class_to_idx

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        # Corrected path joining: filename column already contains the subdirectory
        image_path = os.path.join(self.root_dir, row['filename'])
        image = Image.open(image_path)
        image = self.transform(image)
        # Convert label to int tensor (assuming one label per image)
        label = torch.tensor([self.class_to_idx[row['label']]], dtype=torch.long)
        return image, label

# === DataLoader
dataset = OCRDataset(df, image_dir, transform, class_to_idx)
train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# === ECOCR Model (Simplified CRNN-like)
class ECOCR(nn.Module):
    def __init__(self, num_classes):
        super(ECOCR, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 16x64
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 8x32
        )
        # Adjusted input size for LSTM based on CNN output dimensions
        self.rnn = nn.LSTM(128 * (img_size[0] // 4), 128, num_layers=2, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(128 * 2, num_classes)

    def forward(self, x):
        x = self.cnn(x)  # B, C, H, W
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2)  # B, W, C, H
        x = x.view(b, w, c * h)  # B, W, C*H
        x, _ = self.rnn(x)  # RNN over width
        x = self.fc(x)  # Class scores
        return x

# === Model Init
model = ECOCR(num_classes=len(char_classes))
model = model.to(device)

# === Loss and Optimizer
# Using CrossEntropyLoss for single character classification per image
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# === Training Loop
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for imgs, labels in train_loader:
        imgs = imgs.to(device)
        # Reshape labels for CrossEntropyLoss
        labels = labels.squeeze(1).to(device)


        optimizer.zero_grad()
        outputs = model(imgs)  # B, W, num_classes

        # For single character per image, take the output at the last time step (or average/max)
        # Here, we'll take the output at the last time step for simplicity
        outputs = outputs[:, -1, :] # B, num_classes


        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}")

# === Save Model
torch.save(model.state_dict(), model_path)
print(f" ECOCR Model saved to: {model_path}")
print(f" Class list saved to: {classes_path}")

import cv2
import numpy as np
import os
from google.colab.patches import cv2_imshow

# === Input/output folders
cropped_folder = '/content/cropped_img'  # Folder with cropped plate images
save_folder = '/content/segmented_characters_fq'  # Folder to save character images
os.makedirs(save_folder, exist_ok=True)

# === Character Segmentation Function ===
def segment_characters_from_plate(image_path, prefix):
    print(f"\nProcessing: {image_path}")

    # Load and resize image
    image = cv2.imread(image_path)
    if image is None:
        print(f" Failed to load: {image_path}")
        return
    image = cv2.resize(image, (600, 200))

    # Convert to grayscale and apply threshold
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Morphological operations
    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 1))
    smeared = cv2.dilate(binary, h_kernel, iterations=1)

    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 3))
    smeared = cv2.erode(smeared, v_kernel, iterations=1)

    # Connected component analysis
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(smeared, connectivity=8)

    # === Filtering parameters ===
    min_area = 300         # Ignore small regions
    max_area = 8000
    min_height = 40        # Filter short boxes
    min_width = 10         # Filter very narrow boxes
    max_aspect = 1.5       # Avoid long lines (e.g., dashes or logos)
    characters = []

    for i in range(1, num_labels):  # Skip background
        x, y, w, h, area = stats[i]
        aspect_ratio = w / float(h)

        if min_area <= area <= max_area and w >= min_width and h >= min_height and aspect_ratio <= max_aspect:
            char_img = binary[y:y+h, x:x+w]
            characters.append({'x': x, 'y': y, 'w': w, 'h': h, 'img': char_img})

    # === Line separation and sorting ===
    line_threshold = 50
    lines = []

    for char in characters:
        placed = False
        for line in lines:
            if abs(char['y'] - line[0]['y']) < line_threshold:
                line.append(char)
                placed = True
                break
        if not placed:
            lines.append([char])

    for line in lines:
        line.sort(key=lambda c: c['x'])  # Sort characters in line
    lines.sort(key=lambda l: l[0]['y'])  # Sort lines vertically

    # === Draw and display bounding boxes ===
    for line in lines:
        for char in line:
            cv2.rectangle(image, (char['x'], char['y']), (char['x'] + char['w'], char['y'] + char['h']), (0, 255, 0), 2)
    cv2_imshow(image)

    # === Save character crops ===
    print("Saving characters...")
    char_idx = 1
    for line_idx, line in enumerate(lines):
        for char in line:
            x, y, w, h = char['x'], char['y'], char['w'], char['h']
            char_crop = image[y:y+h, x:x+w]
            cv2_imshow(char_crop)  # Show each character
            save_path = os.path.join(save_folder, f'{prefix}_char_{char_idx:03d}_line{line_idx+1}.jpg')
            cv2.imwrite(save_path, char_crop)
            char_idx += 1

# === Run segmentation for all cropped plates ===
for filename in sorted(os.listdir(cropped_folder)):
    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
        plate_path = os.path.join(cropped_folder, filename)
        prefix = os.path.splitext(filename)[0]
        segment_characters_from_plate(plate_path, prefix)

import os
import torch
import torch.nn as nn
from PIL import Image
from torchvision import transforms

# === Config
segmented_dir = '/content/segmented_characters_fq'
model_path = '/content/ocr_model_ecocr.pth'
classes_path = '/content/ocr_classes.txt'
img_size = (32, 128)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === Load class labels
with open(classes_path, 'r') as f:
    class_labels = [line.strip() for line in f.readlines()]

# === ECOCR Model Definition
class ECOCR(nn.Module):
    def __init__(self, num_classes):
        super(ECOCR, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
        )
        self.rnn = nn.LSTM(128 * (img_size[0] // 4), 128, num_layers=2, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(128 * 2, num_classes)

    def forward(self, x):
        x = self.cnn(x)
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2)
        x = x.view(b, w, c * h)
        x, _ = self.rnn(x)
        x = self.fc(x)
        return x

# === Load model
model = ECOCR(num_classes=len(class_labels))
model.load_state_dict(torch.load(model_path, map_location=device))
model = model.to(device)
model.eval()

# === Image transform
transform = transforms.Compose([
    transforms.Resize(img_size),
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# === Group character images by plate prefix
from collections import defaultdict

plate_groups = defaultdict(list)

for file_name in sorted(os.listdir(segmented_dir)):
    if file_name.endswith('.jpg'):
        prefix = "_".join(file_name.split("_")[:4])  # e.g., bike_plate_crop_0
        plate_groups[prefix].append(file_name)

# === Run OCR for each plate group
for plate_id, char_files in plate_groups.items():
    plate_text = ""
    print(f"\nðŸ§¾ Processing Plate: {plate_id}")

    for file_name in sorted(char_files):  # Ensure characters are in order
        img_path = os.path.join(segmented_dir, file_name)
        img = Image.open(img_path).convert("L")
        img = transform(img).unsqueeze(0).to(device)

        with torch.no_grad():
            output = model(img)
            output = output[:, -1, :]
            pred_idx = output.argmax(dim=1).item()
            predicted_char = class_labels[pred_idx]
            plate_text += predicted_char

        print(f" {file_name} âžœ {predicted_char}")

    print(f" Final Recognized Plate Text [{plate_id}]: {plate_text}")